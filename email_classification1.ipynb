{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf99aef0",
   "metadata": {},
   "source": [
    "# Email Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f772d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RECRUITMENT EMAIL DATASET ===\n",
      "Total emails: 1575\n",
      "Available columns: ['job_application', 'Application for Data Analyst – Amit Sharma\\r\\n\\r\\nHi HR Team,\\r\\n\\r\\nMy name is Amit Sharma, and I am currently working as a Data Analyst at Quantiva Systems. I have over 4 years of experience in data visualization, SQL-based analytics, and building dashboards using Power BI and Tableau.\\r\\n\\r\\nI came across your opening for a Data Analyst, and I believe my background aligns strongly with the role. I`ve handled several reporting automation initiatives and worked closely with cross-functional teams to deliver insights that improved operational decision-making.\\r\\n\\r\\nPlease find my attached résumé for your review. I would be happy to discuss how I can contribute.\\r\\n\\r\\nRegards,\\r\\nAmit Sharma\\r\\nEmail: amit.sharma92@gmail.com\\r\\n\\r\\n\\r\\nPhone: +91-98234-10020']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Load your preprocessed data\n",
    "df = pd.read_csv(\"aws_training_data.csv\")\n",
    "\n",
    "print(\"=== RECRUITMENT EMAIL DATASET ===\")\n",
    "print(f\"Total emails: {len(df)}\")\n",
    "print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ff86a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_application</th>\n",
       "      <th>Application for Data Analyst – Amit Sharma\\r\\n\\r\\nHi HR Team,\\r\\n\\r\\nMy name is Amit Sharma, and I am currently working as a Data Analyst at Quantiva Systems. I have over 4 years of experience in data visualization, SQL-based analytics, and building dashboards using Power BI and Tableau.\\r\\n\\r\\nI came across your opening for a Data Analyst, and I believe my background aligns strongly with the role. I`ve handled several reporting automation initiatives and worked closely with cross-functional teams to deliver insights that improved operational decision-making.\\r\\n\\r\\nPlease find my attached résumé for your review. I would be happy to discuss how I can contribute.\\r\\n\\r\\nRegards,\\r\\nAmit Sharma\\r\\nEmail: amit.sharma92@gmail.com\\r\\n\\r\\n\\r\\nPhone: +91-98234-10020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1575</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>new_requisition</td>\n",
       "      <td>Subject: You’re Selected – UI Designer at Pixe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>516</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        job_application  \\\n",
       "count              1575   \n",
       "unique                5   \n",
       "top     new_requisition   \n",
       "freq                516   \n",
       "\n",
       "       Application for Data Analyst – Amit Sharma\\r\\n\\r\\nHi HR Team,\\r\\n\\r\\nMy name is Amit Sharma, and I am currently working as a Data Analyst at Quantiva Systems. I have over 4 years of experience in data visualization, SQL-based analytics, and building dashboards using Power BI and Tableau.\\r\\n\\r\\nI came across your opening for a Data Analyst, and I believe my background aligns strongly with the role. I`ve handled several reporting automation initiatives and worked closely with cross-functional teams to deliver insights that improved operational decision-making.\\r\\n\\r\\nPlease find my attached résumé for your review. I would be happy to discuss how I can contribute.\\r\\n\\r\\nRegards,\\r\\nAmit Sharma\\r\\nEmail: amit.sharma92@gmail.com\\r\\n\\r\\n\\r\\nPhone: +91-98234-10020  \n",
       "count                                                1575                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "unique                                               1526                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "top     Subject: You’re Selected – UI Designer at Pixe...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "freq                                                    4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c179a66c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\talentprism\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategory\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\talentprism\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\talentprism\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'category'"
     ]
    }
   ],
   "source": [
    "df['category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8316de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\talentprism\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f17884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATSEmailClassifier:\n",
    "    def __init__(self, max_features=8000, max_length=400):\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.num_classes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfef73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def prepare_data(self, texts, labels):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Fit tokenizer\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        # Convert texts to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "\n",
    "        # Encode labels\n",
    "        y = self.label_encoder.fit_transform(labels)\n",
    "        self.num_classes = len(np.unique(y))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef30e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build_model(self):\n",
    "        \"\"\"Build the neural network model\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, 128, input_length=self.max_length),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbf867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, texts, labels, validation_split=0.2, epochs=25, batch_size=16):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        X, y = self.prepare_data(texts, labels)\n",
    "\n",
    "        # Build model\n",
    "        self.build_model()\n",
    "\n",
    "        # Compute class weights to handle imbalance\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y),\n",
    "            y=y\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        print(f\"Training model with {self.num_classes} classes...\")\n",
    "        print(f\"Vocabulary size: {len(self.tokenizer.word_index)}\")\n",
    "        print(f\"Sample distribution: {Counter(y)}\")\n",
    "\n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e308bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, texts):\n",
    "        \"\"\"Predict email categories\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return self.label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "    def predict_proba(self, texts):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate(self, texts, labels):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "        y = self.label_encoder.transform(labels)\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y, predicted_classes))\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y, predicted_classes)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.label_encoder.classes_,\n",
    "                   yticklabels=self.label_encoder.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return classification_report(y, predicted_classes, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de6934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        self.model.save(f\"{filepath}_model.h5\")\n",
    "        with open(f\"{filepath}_tokenizer.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        with open(f\"{filepath}_label_encoder.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.label_encoder, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        self.model = tf.keras.models.load_model(f\"{filepath}_model.h5\")\n",
    "        with open(f\"{filepath}_tokenizer.pkl\", 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        with open(f\"{filepath}_label_encoder.pkl\", 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a9cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_emails(df):\n",
    "    \"\"\"Categorize emails based on content patterns with fallback\"\"\"\n",
    "    def classify_email(text):\n",
    "        if pd.isna(text):\n",
    "            return 'other'\n",
    "\n",
    "        text_lower = str(text).lower()\n",
    "\n",
    "        # Define keyword patterns for each category\n",
    "        patterns = {\n",
    "            'candidate_application': ['application', 'applied', 'resume', 'cv', 'portfolio', 'candidate profile'],\n",
    "            'candidate_availability': ['availability', 'schedule', 'time', 'when', 'flexible', 'available'],\n",
    "            'candidate_response': ['response', 'reply', 'answering', 'responding', 'acknowledging'],\n",
    "            'interview_scheduling': ['interview', 'meeting', 'schedule', 'set up', 'arrange', 'book', 'appointment'],\n",
    "            'interview_feedback': ['feedback', 'review', 'after', 'post', 'interview feedback'],\n",
    "            'interview_confirmation': ['confirm', 'confirmed', 'yes', 'ok', 'accepted', 'confirmed interview'],\n",
    "            'offer_sent': ['offer', 'salary', 'package', 'compensation', 'proposed', 'position offer'],\n",
    "            'offer_response': ['accept', 'accepted', 'decline', 'declined', 'negotiate', 'negotiation'],\n",
    "            'client_submission': ['submit', 'send', 'forward', 'present', 'candidate submission'],\n",
    "            'client_feedback': ['feedback', 'review', 'comment', 'client feedback', 'client response'],\n",
    "            'new_requisition': ['new', 'open', 'requirement', 'job requisition', 'position opening'],\n",
    "            'onboarding': ['onboarding', 'documents', 'start date', 'joining', 'orientation'],\n",
    "            'internal_coordination': ['internal', 'team', 'colleagues', 'coordinator', 'internal email'],\n",
    "            'system_notification': ['system', 'automated', 'notification', 'email notification'],\n",
    "            'general_inquiry': ['hello', 'hi', 'greeting', 'inquiry', 'question', 'help'],\n",
    "            'spam': ['spam', 'unsubscribe', 'advertisement', 'promotion', 'marketing'],\n",
    "        }\n",
    "\n",
    "        # Check each pattern\n",
    "        for category, keywords in patterns.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                return category\n",
    "\n",
    "        # Default to general inquiry if no specific pattern matches\n",
    "        return 'general_inquiry'\n",
    "\n",
    "    # Apply classification\n",
    "    df['category'] = df['full_preprocessed'].apply(classify_email)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3caaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rare_categories(df, min_samples=5):\n",
    "    \"\"\"Filter out categories with very few samples\"\"\"\n",
    "    category_counts = df['category'].value_counts()\n",
    "    valid_categories = category_counts[category_counts >= min_samples].index\n",
    "    filtered_df = df[df['category'].isin(valid_categories)].copy()\n",
    "\n",
    "    print(f\"Original categories: {len(category_counts)}\")\n",
    "    print(f\"Valid categories (≥{min_samples} samples): {len(valid_categories)}\")\n",
    "    print(f\"Samples kept: {len(filtered_df)} out of {len(df)}\")\n",
    "    print(\"\\nCategory distribution after filtering:\")\n",
    "    print(filtered_df['category'].value_counts())\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e6d4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(df):\n",
    "    \"\"\"Train the ATS email classifier\"\"\"\n",
    "    # Categorize emails\n",
    "    df = categorize_emails(df)\n",
    "\n",
    "    # Filter out rare categories\n",
    "    df_filtered = filter_rare_categories(df, min_samples=3)\n",
    "\n",
    "    if len(df_filtered) < 10:  # Need minimum samples for training\n",
    "        print(\"Not enough data after filtering. Using simplified approach...\")\n",
    "        # Fallback: group rare categories\n",
    "        category_counts = df['category'].value_counts()\n",
    "        threshold = 5\n",
    "        df['category'] = df['category'].apply(\n",
    "            lambda x: 'other' if category_counts[x] < threshold else x\n",
    "        )\n",
    "        df_filtered = df[df['category'] != 'other']  # Remove 'other' for training\n",
    "\n",
    "        if len(df_filtered) < 10:\n",
    "            df_filtered = df  # Use all data if still too small\n",
    "\n",
    "    # Prepare data\n",
    "    texts = df_filtered['full_preprocessed'].fillna('').tolist()\n",
    "    labels = df_filtered['category'].tolist()\n",
    "\n",
    "    # Check if we have enough samples for each class\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    min_count = min(counts)\n",
    "\n",
    "    if min_count < 2:\n",
    "        print(\"Some classes have only 1 sample. Using stratified split with validation split = 0.1\")\n",
    "        validation_split = 0.1\n",
    "    else:\n",
    "        validation_split = 0.2 if len(texts) > 50 else 0.1\n",
    "\n",
    "    # Split data ensuring minimum samples per class\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.2, random_state=42,\n",
    "            stratify=labels if min_count >= 2 else None\n",
    "        )\n",
    "    except:\n",
    "        # Fallback if stratification fails\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "    # Initialize and train classifier\n",
    "    classifier = ATSEmailClassifier(max_features=8000, max_length=400)\n",
    "\n",
    "    print(f\"\\n=== TRAINING ATS EMAIL CLASSIFIER ===\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Unique categories: {len(set(y_train))}\")\n",
    "\n",
    "    history = classifier.train(X_train, y_train, epochs=30, batch_size=16)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(f\"\\n=== EVALUATION RESULTS ===\")\n",
    "    classifier.evaluate(X_test, y_test)\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71f22993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reply_templates():\n",
    "    \"\"\"Create automated reply templates based on categories\"\"\"\n",
    "    reply_templates = {\n",
    "        'candidate_application': \"Thank you for your application. We have received your resume and will review it. We'll get back to you within 3-5 business days.\",\n",
    "        'candidate_availability': \"Thank you for providing your availability. We'll schedule your interview soon and send you the details.\",\n",
    "        'candidate_response': \"Thank you for your response. We appreciate your interest in this position.\",\n",
    "        'interview_scheduling': \"We'd like to schedule your interview. Please confirm your availability for the proposed time.\",\n",
    "        'interview_feedback': \"Thank you for attending the interview. We'll provide feedback soon.\",\n",
    "        'interview_confirmation': \"Thank you for confirming your interview. We look forward to meeting you.\",\n",
    "        'offer_sent': \"Congratulations! We'd like to offer you the position. Please review the offer details and let us know your decision.\",\n",
    "        'offer_response': \"Thank you for your response regarding the offer. We'll proceed accordingly.\",\n",
    "        'client_submission': \"We have submitted the candidate profile to the client. We'll update you on their feedback.\",\n",
    "        'client_feedback': \"Thank you for the client feedback. We'll take it into consideration.\",\n",
    "        'new_requisition': \"Thank you for the new job requisition. We'll start sourcing suitable candidates immediately.\",\n",
    "        'onboarding': \"Please complete the onboarding documents attached. Let us know if you need any assistance.\",\n",
    "        'internal_coordination': \"Internal coordination email processed successfully.\",\n",
    "        'system_notification': \"System notification processed.\",\n",
    "        'general_inquiry': \"Thank you for your inquiry. We'll address your query and get back to you soon.\",\n",
    "        'spam': \"This email has been marked as spam and will be handled accordingly.\",\n",
    "        'other': \"Thank you for your message. We'll review it and respond accordingly.\"\n",
    "    }\n",
    "    return reply_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39687bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load your data\n",
    "    df = pd.read_csv('aws_training_data.csv')\n",
    "\n",
    "    # Rename columns: assume second column is the email text\n",
    "    df = df.rename(columns={df.columns[1]: 'full_preprocessed'})\n",
    "    df = df[['full_preprocessed']]  # Keep only the text column\n",
    "\n",
    "    print(\"=== DATASET LOADED ===\")\n",
    "    print(f\"Total emails: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Remove any rows with missing full_preprocessed text\n",
    "    df = df.dropna(subset=['full_preprocessed'])\n",
    "    print(f\"After cleaning: {len(df)} emails\")\n",
    "\n",
    "    # Train the classifier\n",
    "    classifier = train_classifier(df)\n",
    "\n",
    "    # Save the model\n",
    "    classifier.save_model(\"ats_email_classifier\")\n",
    "\n",
    "    # Create reply templates\n",
    "    reply_templates = create_reply_templates()\n",
    "\n",
    "    # Test with sample emails\n",
    "    test_emails = [\n",
    "        \"I have attached my resume for the software engineer position. Please let me know if you need any additional information.\",\n",
    "        \"I'm available for an interview next week on Tuesday or Wednesday. Please let me know what works for you.\",\n",
    "        \"Thank you for the interview opportunity. I would like to accept your offer for the position.\",\n",
    "        \"We have reviewed the candidate's profile and would like to schedule an interview next week.\",\n",
    "        \"Please find the attached onboarding documents for the new hire.\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== TEST PREDICTIONS AND AUTOMATED REPLIES ===\")\n",
    "    for i, email in enumerate(test_emails, 1):\n",
    "        print(f\"\\n--- Test Email {i} ---\")\n",
    "        print(f\"Email: {email[:100]}...\")\n",
    "        reply = generate_automated_reply(classifier, email, reply_templates)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return classifier, reply_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88174e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.venv', 'aws_training_data.csv', 'email_classification.ipynb', 'email_classification1.ipynb', 'email_classification_2.ipynb', 'email_classification_3.ipynb', 'email_classification_bert.ipynb', 'final_model', 'logs', 'README.md', 'results']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\".\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
