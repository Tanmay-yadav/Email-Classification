{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== COMPLETE DISTILBERT-BASED ATS EMAIL CLASSIFIER =====================\n",
    "# Copyâ€“Paste Ready | Small Model | Resume Training | No Optimizer Bloat\n",
    "# ========================================================================================\n",
    "\n",
    "# ---------- STEP 0: IMPORT LIBRARIES ----------\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    TFDistilBertForSequenceClassification\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "\n",
    "# ---------- STEP 1: LOAD & CLEAN DATA ----------\n",
    "df = pd.read_csv(\"E:\\\\Talentprism\\\\data\\\\final_training_data_with_missing_classes.csv\")\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "df = df.dropna(subset=[\"label\", \"text\"])\n",
    "df = df.drop_duplicates(subset=[\"text\"])\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"Original Class Distribution:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# ---------- STEP 2: BALANCE DATASET ----------\n",
    "df_balanced = df.groupby(\"label\").apply(\n",
    "    lambda x: x.sample(min(len(x), 450), random_state=42)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nBalanced Class Distribution:\")\n",
    "print(df_balanced[\"label\"].value_counts())\n",
    "\n",
    "# ---------- STEP 3: LABEL ENCODING ----------\n",
    "label_encoder = LabelEncoder()\n",
    "df_balanced[\"label_encoded\"] = label_encoder.fit_transform(df_balanced[\"label\"])\n",
    "\n",
    "num_classes = df_balanced[\"label_encoded\"].nunique()\n",
    "print(\"\\nClasses:\", label_encoder.classes_)\n",
    "\n",
    "# ---------- STEP 4: TRAIN / VAL / TEST SPLIT ----------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_balanced[\"text\"],\n",
    "    df_balanced[\"label_encoded\"],\n",
    "    test_size=0.3,\n",
    "    stratify=df_balanced[\"label_encoded\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.33,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# ---------- STEP 5: LOAD DISTILBERT TOKENIZER ----------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "\n",
    "# ---------- STEP 6: TOKENIZATION ----------\n",
    "def encode_texts(texts, tokenizer, max_len=256):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(X_train, tokenizer)\n",
    "val_enc   = encode_texts(X_val, tokenizer)\n",
    "test_enc  = encode_texts(X_test, tokenizer)\n",
    "\n",
    "# ---------- STEP 7: CLASS WEIGHTS ----------\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"\\nClass Weights:\", class_weight_dict)\n",
    "\n",
    "# ---------- STEP 8: LOAD DISTILBERT MODEL ----------\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_classes,\n",
    "    from_pt=True\n",
    ")\n",
    "\n",
    "# ---------- STEP 9: COMPILE MODEL ----------\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# ---------- STEP 10: CALLBACKS ----------\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.3,\n",
    "    patience=1,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, \"ckpt\"),\n",
    "    save_weights_only=True,     # IMPORTANT\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- STEP 11: RESUME FROM CHECKPOINT IF EXISTS ----------\n",
    "latest_ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_ckpt:\n",
    "    print(f\"Resuming from checkpoint: {latest_ckpt}\")\n",
    "    model.load_weights(latest_ckpt)\n",
    "\n",
    "# ---------- STEP 12: TRAIN MODEL ----------\n",
    "history = model.fit(\n",
    "    dict(train_enc),\n",
    "    y_train,\n",
    "    validation_data=(dict(val_enc), y_val),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[checkpoint_cb, early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- STEP 13: EVALUATE ----------\n",
    "test_preds = model.predict(dict(test_enc))\n",
    "y_pred = np.argmax(test_preds.logits, axis=1)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# ---------- STEP 14: CONFUSION MATRIX ----------\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- STEP 15: TRAINING CURVES ----------\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------- STEP 16: CONFIDENCE-AWARE PREDICTION ----------\n",
    "def predict_email(text, threshold=0.7):\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"tf\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    logits = model(enc).logits\n",
    "    probs = tf.nn.softmax(logits, axis=1)\n",
    "    idx = tf.argmax(probs, axis=1).numpy()[0]\n",
    "    confidence = float(probs[0][idx])\n",
    "\n",
    "    result = {\n",
    "        \"predicted_class\": label_encoder.classes_[idx],\n",
    "        \"confidence\": confidence\n",
    "    }\n",
    "\n",
    "    if confidence < threshold:\n",
    "        result[\"warning\"] = \"Low confidence prediction\"\n",
    "\n",
    "    return result\n",
    "\n",
    "print(predict_email(\"Dear HR, I am applying for the Data Scientist position.\"))\n",
    "\n",
    "# ---------- STEP 17: SAVE FINAL ARTIFACTS ----------\n",
    "model.save_weights(\"ats_distilbert_weights\")      # ~250 MB\n",
    "tokenizer.save_pretrained(\"ats_tokenizer\")        # <5 MB\n",
    "\n",
    "# ===================== END OF COMPLETE CODE =====================\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
