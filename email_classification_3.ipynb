{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcdcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4537ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aws_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['job_application'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------------------\n",
    "# TEXT CLEANING\n",
    "# ----------------------------\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"aws_training_data.csv\")\n",
    "# df[\"email_text\"] = df[\"email_text\"].astype(str).apply(clean_text)\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# ----------------------------\n",
    "# ENCODE LABELS\n",
    "# ----------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"category\"])\n",
    "\n",
    "# ----------------------------\n",
    "# TOKENIZATION\n",
    "# ----------------------------\n",
    "MAX_WORDS = 8000\n",
    "MAX_LEN = 300\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"email_text\"])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df[\"email_text\"])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)\n",
    "\n",
    "# ----------------------------\n",
    "# TRAIN / TEST SPLIT\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# HANDLE CLASS IMBALANCE\n",
    "# ----------------------------\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"\\nClass Weights:\", class_weights)\n",
    "\n",
    "# ----------------------------\n",
    "# BUILD MODEL\n",
    "# ----------------------------\n",
    "model = Sequential([\n",
    "    Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(label_encoder.classes_), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ----------------------------\n",
    "# TRAIN MODEL\n",
    "# ----------------------------\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# EVALUATION\n",
    "# ----------------------------\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# ----------------------------\n",
    "# CONFUSION MATRIX\n",
    "# ----------------------------\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# SAVE MODEL\n",
    "# ----------------------------\n",
    "model.save(\"email_classifier.h5\")\n",
    "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))\n",
    "pickle.dump(label_encoder, open(\"label_encoder.pkl\", \"wb\"))\n",
    "\n",
    "print(\"\\nModel saved successfully!\")\n",
    "\n",
    "# ----------------------------\n",
    "# MANUAL EMAIL TESTING\n",
    "# ----------------------------\n",
    "def predict_email(email):\n",
    "    email = clean_text(email)\n",
    "    seq = tokenizer.texts_to_sequences([email])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    pred = model.predict(padded)\n",
    "    label = label_encoder.inverse_transform([np.argmax(pred)])\n",
    "    confidence = np.max(pred)\n",
    "    return label[0], confidence\n",
    "\n",
    "# ----------------------------\n",
    "# AUTOMATED REPLIES\n",
    "# ----------------------------\n",
    "reply_templates = {\n",
    "    \"new_requisition\": \"Thank you for the new job requisition. We will begin sourcing candidates.\",\n",
    "    \"interview_scheduling\": \"Your interview will be scheduled shortly. We will share the details.\",\n",
    "    \"candidate_selection\": \"The candidate has been shortlisted. We will proceed further.\",\n",
    "    \"job_application\": \"Thank you for your application. Our team will review it shortly.\",\n",
    "    \"spam\": \"This email has been identified as spam.\"\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# TEST EMAIL\n",
    "# ----------------------------\n",
    "test_email = \"I want to apply for data analyst role. Offer offer buy car cheap.\"\n",
    "\n",
    "category, confidence = predict_email(test_email)\n",
    "\n",
    "print(\"\\nManual Test Email:\")\n",
    "print(test_email)\n",
    "print(\"Predicted Category:\", category)\n",
    "print(\"Confidence:\", round(confidence, 2))\n",
    "print(\"Auto Reply:\", reply_templates.get(category))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfcacd7",
   "metadata": {},
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662366fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499020e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aws_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# cretate columns for data analysis \n",
    "df.describe()\n",
    "df.value_counts()\n",
    "df['job_application'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f48fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to message for spam email and job application email give me method to do that\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8137a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the old file. \n",
    "# Since your example shows the label first, we assign names 'label' and 'text'\n",
    "old_df = pd.read_csv(\"aws_training_data.csv\", names=[\"label\", \"text\"], header=None)\n",
    "\n",
    "# 2. Load the new synthetic file (which already has headers)\n",
    "new_df = pd.read_csv(\"email_dataset.csv\")\n",
    "\n",
    "# 3. Combine them\n",
    "combined_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "\n",
    "# 4. Shuffle to mix the old and new data together\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 5. Save the final version\n",
    "combined_df.to_csv(\"final_training_data.csv\", index=False)\n",
    "\n",
    "print(f\"Merge complete! Total training samples: {len(combined_df)}\")\n",
    "print(combined_df.head()) # Preview the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1577f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.value_counts()\n",
    "df.info()\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e79610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Enron CSV...\n",
      "Stripping threads and cleaning...\n",
      "Removed 289923 duplicate emails.\n",
      "Successfully saved 935 balanced, unique rows to cleaned_ats_enron.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "def clean_enron_professional(enron_emails, cleaned_ats_enron):\n",
    "    print(\"Loading Enron CSV...\")\n",
    "    df = pd.read_csv(enron_emails)\n",
    "\n",
    "    def advanced_clean(msg):\n",
    "        # 1. Split Header from Body\n",
    "        parts = msg.split('\\n\\n', 1)\n",
    "        body = parts[1] if len(parts) > 1 else parts[0]\n",
    "        \n",
    "        # 2. Remove Forwarded/Original Message Chains\n",
    "        # This stops the model from reading the same old text over and over\n",
    "        body = re.split(r'-----Original Message-----|From:|>|To:', body)[0]\n",
    "        \n",
    "        # 3. Basic text cleaning\n",
    "        body = body.lower()\n",
    "        body = re.sub(r'\\s+', ' ', body) # Remove extra whitespace/newlines\n",
    "        return body.strip()\n",
    "\n",
    "    print(\"Stripping threads and cleaning...\")\n",
    "    df['clean_text'] = df['message'].apply(advanced_clean)\n",
    "\n",
    "    # 4. De-duplication: Remove exact duplicates\n",
    "    # Many emails are the same but have different Message-IDs\n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['clean_text'])\n",
    "    print(f\"Removed {initial_count - len(df)} duplicate emails.\")\n",
    "\n",
    "    # 5. Labeling Logic (Optimized for HR/ATS)\n",
    "    def assign_label(text):\n",
    "        # Job Application\n",
    "        if any(w in text for w in ['resume attached', 'applying for', 'cv', 'cover letter']):\n",
    "            return 'job_application'\n",
    "        # Interview Scheduling\n",
    "        if any(w in text for w in ['interview', 'availability', 'schedule', 'calendar invitation']):\n",
    "            return 'interview_scheduling'\n",
    "        # New Requisition\n",
    "        if any(w in text for w in ['open req', 'requisition', 'headcount', 'position approval']):\n",
    "            return 'new_requisition'\n",
    "        # Candidate Selection\n",
    "        if any(w in text for w in ['finalist', 'shortlist', 'hired', 'selection', 'background check']):\n",
    "            return 'candidate_selection'\n",
    "        # Spam\n",
    "        if any(w in text for w in ['win', 'prize', 'viagra', 'account suspended', 'click here']):\n",
    "            return 'spam'\n",
    "        return 'other'\n",
    "\n",
    "    df['label'] = df['clean_text'].apply(assign_label)\n",
    "    \n",
    "    # 6. Final Filter and Sample\n",
    "    final_df = df[df['label'] != 'other'][['clean_text', 'label']]\n",
    "    \n",
    "    # Ensure classes are balanced\n",
    "    min_count = final_df['label'].value_counts().min()\n",
    "    balanced_df = final_df.groupby('label').head(min_count)\n",
    "\n",
    "    balanced_df.to_csv(cleaned_ats_enron, index=False)\n",
    "    print(f\"Successfully saved {len(balanced_df)} balanced, unique rows to {cleaned_ats_enron}\")\n",
    "\n",
    "# To execute:\n",
    "clean_enron_professional('enron_emails.csv', 'cleaned_ats_enron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0138fb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>randy, can you send me a schedule of the salar...</td>\n",
       "      <td>interview_scheduling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please cc the following distribution list with...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>---------------------- forwarded by phillip k ...</td>\n",
       "      <td>interview_scheduling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reagan, just wanted to give you an update. i h...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nymex expiration is during this time frame. pl...</td>\n",
       "      <td>interview_scheduling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text                 label\n",
       "0  randy, can you send me a schedule of the salar...  interview_scheduling\n",
       "1  please cc the following distribution list with...                  spam\n",
       "2  ---------------------- forwarded by phillip k ...  interview_scheduling\n",
       "3  reagan, just wanted to give you an update. i h...                  spam\n",
       "4  nymex expiration is during this time frame. pl...  interview_scheduling"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_ats_enron.csv')\n",
    "df.head()\n",
    "# df['label'].value_counts()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d366c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].index==\u001b[32m1\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "if(df['label'].index==1):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc690b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\talentprism\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['clean_text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_2688\\1593636322.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    214\u001b[39m \n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Load data from CSV (assuming columns are 'category' and the email text column; adjust if needed)\u001b[39;00m\n\u001b[32m    216\u001b[39m df = pd.read_csv(\u001b[33m'cleaned_ats_enron.csv'\u001b[39m)\n\u001b[32m    217\u001b[39m df.columns = [\u001b[33m'label'\u001b[39m, \u001b[33m'clean_text      '\u001b[39m]  \u001b[38;5;66;03m# Rename if necessary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m df = df.dropna(subset=[\u001b[33m'clean_text'\u001b[39m])\n\u001b[32m    219\u001b[39m texts = df[\u001b[33m'clean_text'\u001b[39m].tolist()\n\u001b[32m    220\u001b[39m labels = df[\u001b[33m'label'\u001b[39m].tolist()\n\u001b[32m    221\u001b[39m \n",
      "\u001b[32me:\\talentprism\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6688\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6689\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6690\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6691\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6692\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6693\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6694\u001b[39m \n\u001b[32m   6695\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['clean_text']"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# class ImprovedATSEmailClassifier:\n",
    "#     def __init__(self, max_features=10000, max_length=500):\n",
    "#         self.max_features = max_features\n",
    "#         self.max_length = max_length\n",
    "#         self.tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "#         self.label_encoder = LabelEncoder()\n",
    "#         self.model = None\n",
    "#         self.num_classes = None\n",
    "\n",
    "#     def preprocess_text(self, text):\n",
    "#         \"\"\"Clean text: lowercase, remove punctuation, stop words.\"\"\"\n",
    "#         text = text.lower()\n",
    "#         text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "#         text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "#         return text\n",
    "\n",
    "#     def prepare_data(self, texts, labels):\n",
    "#         \"\"\"Prepare data with preprocessing.\"\"\"\n",
    "#         texts = [self.preprocess_text(t) for t in texts]\n",
    "#         # self.tokenizer.fit_on_texts(texts)\n",
    "#         if is_training:\n",
    "#             self.tokenizer.fit_on_texts(texts)\n",
    "#         sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "#         X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "#         y = self.label_encoder.fit_transform(labels)\n",
    "#         self.num_classes = len(np.unique(y))\n",
    "#         return X, y\n",
    "\n",
    "#     def build_model(self):\n",
    "#         \"\"\"Improved model: Bidirectional LSTM for better sequence handling.\"\"\"\n",
    "#         model = Sequential([\n",
    "#             Embedding(self.max_features, 128, input_length=self.max_length),\n",
    "#             Bidirectional(LSTM(64, return_sequences=True)),  # Better for context\n",
    "#             GlobalMaxPooling1D(),\n",
    "#             Dense(128, activation='relu'),\n",
    "#             Dropout(0.5),\n",
    "#             Dense(64, activation='relu'),\n",
    "#             Dropout(0.5),\n",
    "#             Dense(self.num_classes, activation='softmax')\n",
    "#         ])\n",
    "#         model.compile(\n",
    "#             optimizer='adam',\n",
    "#             loss='sparse_categorical_crossentropy',\n",
    "#             metrics=['accuracy']\n",
    "#         )\n",
    "#         self.model = model\n",
    "#         return model\n",
    "\n",
    "#     def train(self, texts, labels, validation_split=0.2, epochs=10, batch_size=32):\n",
    "#         X, y = self.prepare_data(texts, labels)\n",
    "#         self.build_model()\n",
    "        \n",
    "#         # Class weights for imbalance\n",
    "#         class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "#         class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "#         # Early stopping to prevent overfitting\n",
    "#         early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "        \n",
    "#         history = self.model.fit(\n",
    "#             X, y,\n",
    "#             validation_split=validation_split,\n",
    "#             epochs=10,\n",
    "#             batch_size=batch_size,\n",
    "#             class_weight=class_weight_dict,\n",
    "#             callbacks=[early_stop],\n",
    "#             verbose=1\n",
    "#         )\n",
    "#         return history\n",
    "\n",
    "#     def predict(self, texts):\n",
    "#         texts = [self.preprocess_text(t) for t in texts]\n",
    "#         sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "#         X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "#         predictions = self.model.predict(X)\n",
    "#         predicted_classes = np.argmax(predictions, axis=1)\n",
    "#         return self.label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "#     def evaluate(self, texts, labels):\n",
    "#         texts = [self.preprocess_text(t) for t in texts]\n",
    "#         sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "#         X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "#         y = self.label_encoder.transform(labels)\n",
    "#         predictions = self.model.predict(X)\n",
    "#         predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "#         print(\"Classification Report:\")\n",
    "#         print(classification_report(y, predicted_classes, target_names=self.label_encoder.classes_))\n",
    "        \n",
    "#         cm = confusion_matrix(y, predicted_classes)\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                    xticklabels=self.label_encoder.classes_,\n",
    "#                    yticklabels=self.label_encoder.classes_)\n",
    "#         plt.title('Confusion Matrix')\n",
    "#         plt.ylabel('Actual')\n",
    "#         plt.xlabel('Predicted')\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.yticks(rotation=0)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "class ImprovedATSEmailClassifier:\n",
    "    def __init__(self, max_features=10000, max_length=500):\n",
    "        self.max_features = max_features\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean text: lowercase, remove punctuation, stop words.\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        return text\n",
    "\n",
    "    def prepare_data(self, texts, labels=None, is_training=True):\n",
    "        \"\"\"Prepare data with preprocessing. Added is_training logic.\"\"\"\n",
    "        processed_texts = [self.preprocess_text(t) for t in texts]\n",
    "        \n",
    "        # FIX: Only fit the dictionary during the training phase\n",
    "        if is_training:\n",
    "            self.tokenizer.fit_on_texts(processed_texts)\n",
    "        \n",
    "        sequences = self.tokenizer.texts_to_sequences(processed_texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length)\n",
    "        \n",
    "        if labels is not None:\n",
    "            if is_training:\n",
    "                y = self.label_encoder.fit_transform(labels)\n",
    "                self.num_classes = len(np.unique(y))\n",
    "            else:\n",
    "                y = self.label_encoder.transform(labels)\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Improved model architecture with higher Dropout to fight overfitting.\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, 128, input_length=self.max_length),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5), # Forces model to ignore some keywords\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5), # Increased to prevent memorization\n",
    "            Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, texts, labels, validation_split=0.2, epochs=10, batch_size=32):\n",
    "        # Pass is_training=True here\n",
    "        X, y = self.prepare_data(texts, labels, is_training=True)\n",
    "        self.build_model()\n",
    "        \n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        # Early stopping: Stops if the model stops actually learning\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=3, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def predict(self, texts):\n",
    "        # Pass is_training=False to keep the tokenizer dictionary locked\n",
    "        X = self.prepare_data(texts, labels=None, is_training=False)\n",
    "        predictions = self.model.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        return self.label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "    def evaluate(self, texts, labels):\n",
    "        # Pass is_training=False for evaluation\n",
    "        X, y = self.prepare_data(texts, labels, is_training=False)\n",
    "        predictions = self.model.predict(X)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        print(classification_report(y, predicted_classes, target_names=self.label_encoder.classes_))\n",
    "\n",
    "# Load data from CSV (assuming columns are 'category' and the email text column; adjust if needed)\n",
    "df = pd.read_csv('cleaned_ats_enron.csv')\n",
    "df.columns = ['label', 'clean_text\t']  # Rename if necessary\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "texts = df['clean_text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Train model\n",
    "classifier = ImprovedATSEmailClassifier()\n",
    "history = classifier.train(X_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate\n",
    "classifier.evaluate(X_test, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Test prediction\n",
    "test_email = \"I am applying for the data analyst position. Attached is my resume.\"\n",
    "print(f\"Predicted Category: {classifier.predict([test_email])[0]}\")\n",
    "test_email = \"I am applying for the offer offer buy this car get a gift free. Attached is my resume.\"\n",
    "print(f\"Predicted Category: {classifier.predict([test_email])[0]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf693c1",
   "metadata": {},
   "source": [
    "# Merging the enron cleaned data and the final_ats_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
